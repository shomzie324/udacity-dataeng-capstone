{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration and Tempurature Examination\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from pyspark.sql.functions import *\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "This project will focus on exploring the relationship between tempurature and immigration in a specific state such as New York or city such as new york city. This will require using only the portions of the US Immigration Data in which the arrival was in a city in new york. To supplement this data, US tempurature data will also be used. In addition, demographic data will be added to explore relationships if any between tempurature and the frequency of immigration within a specifc demographic.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in immigration data\t\n",
    "# note: too large to read everything into memory at once\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check schema of df \n",
    "print(df_spark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pd = pd.DataFrame(df_spark.take(5))\n",
    "im_pd.columns = df_spark.columns\n",
    "im_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read demographic data\n",
    "dem_df = pd.read_csv(\"us-cities-demographics.csv\", sep=\";\")\n",
    "dem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Tempurature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in tempurature data\n",
    "temp_df = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airport Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read airport codes\n",
    "air_codes_df = pd.read_csv(\"airport-codes_csv.csv\")\n",
    "air_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_codes_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data\n",
    "* Identify data quality issues, like missing values, duplicate data, etc.\n",
    "* Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Immigration Data Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume\n",
    "Since the data analysis is focused on the relationship between tempurature and immigration in new york city , the exploration will be focused on that subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_nyc = df_spark.filter(df_spark.i94port == \"NYC\")\n",
    "df_spark_nyc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets take a look at the columns again just for the new york city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_nyc.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Demographic Data Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new york city data and check volume\n",
    "# dem_df_nyc = dem_df[dem_df['City'] == \"New York\"]\n",
    "# dem_df_nyc.count()\n",
    "dem_df.columns = ['city', 'state', 'median_age', 'male_population','female_population',\n",
    "                     'total_population', 'number_of_veterans','foreign_born', 'average_household_size',\n",
    "                     'state_code','race','count']\n",
    "dem_df_spark = spark.createDataFrame(dem_df)\n",
    "dem_count = dem_df_spark.count()\n",
    "print(dem_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, I will take a look at the demographic city data to explore what data is there and if there are any issues with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df_spark.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this project, I scoped my analysis to a single city so I will only need the few records that pertain to New York, New York specifically, but I will still assess the data as whole as it pertains to duplicates and missing fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_na_count = dem_df_spark.dropna(how=\"any\").count()\n",
    "dem_dup_count = dem_df_spark.dropDuplicates().count()\n",
    "\n",
    "print(f\"Number of records with missing data in demographics data set: {dem_count - dem_na_count}\")\n",
    "print(f\"Number of records with duplicate data in demographics data set: {dem_count - dem_dup_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Tempurature Data Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nyc sample\n",
    "temp_df_nyc = spark.createDataFrame(temp_df[temp_df['City'] == \"New York\"])\n",
    "temp_count = temp_df_nyc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tempurature sample\n",
    "temp_df_nyc.toPandas().head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will assess how many records are duplicates or have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_na_count = temp_df_nyc.dropna(how=\"any\").count()\n",
    "temp_dup_count = temp_df_nyc.dropDuplicates().count()\n",
    "\n",
    "print(f\"Number of records with missing data in demographics data set: {temp_count - temp_na_count}\")\n",
    "print(f\"Number of records with duplicate data in demographics data set: {temp_count - temp_dup_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airport Code Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though airport codes are not needed for the scope of this project, it will be processed anyways in case future analysis requires it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before the data can be used with the spark, thedata types need to be turned into something spark can parse, as currently everything was read in as object, for now everything will be made to be a string and adjusted to their proper type in the cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(air_codes_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_codes_df = air_codes_df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_codes_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count air code data\n",
    "air_df = spark.createDataFrame(air_codes_df)\n",
    "air_count = air_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(air_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first I will look at the data just to remind us of what is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the Data Model\n",
    "## 3.1 Conceptual Data Model\n",
    "(Map out the conceptual data model and explain why you chose that model)\n",
    "\n",
    "The Data Model that will be used is a star schema. This is because since there will be so much data in the data warehouse, it is important to simplify the joins as not doing so will significantly slow down queries. \n",
    "\n",
    "The us immigration data will serve as the fact table since that is the primary event in question i.e people immigrating into the US. The demographic data will be a dimension table and should join with the immigration data via the city name. The Tempurature data will be another dimension table which can join with the immigration data based on the date of arrival. The airport codes can be joined based on the city name.\n",
    "\n",
    "## 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "Currently the data exists as csv files and the end goal is to have a datawarehouse that can be quried. AWS Will be used for the pipeline and the steps are as follows:\n",
    "\n",
    "1. extract the source data csv files using spark and examine and report and the data quality\n",
    "2. clean the data as per the above cleaning transformation requirements\n",
    "3. set up AWS using IAC and create the needed S3 buckets for the data lake and the tables needed for redshift\n",
    "4. load the cleaned spark dataframes into S3 as csv files\n",
    "5. use S3 to load data into redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "## 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the confugration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up clients for EC2, S3, IAM and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use S3 client to create a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket\n",
    "try:\n",
    "    location = {'LocationConstraint': \"us-west-2\"}\n",
    "    s3.create_bucket(Bucket=\"deng-capstone\", \\\n",
    "    CreateBucketConfiguration=location)\n",
    "except ClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List buckets to ensure it was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deng_bucket = s3.Bucket(\"deng-capstone\")\n",
    "\n",
    "# Output the bucket object names\n",
    "for obj in deng_bucket.objects.filter():\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IAM Role for redshift to S3 access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach a policy\n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get and print the IAM role ARN to make sure it worked\n",
    "print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create redshift cluster with provided configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display cluster info to see when it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "# get 1st cluster from cluster list (only 1 in this case)\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When cluster is ready, get the info needed to connect to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wait Until Redshift is available before running this\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open an incoming  TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connection to redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When ready, delete cluster (Do not do this until project run through is complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction\n",
    "Since the data has already been retreived so that it could be explored and assessed in a previous step the extraction step is already handled. All that is left is to transform and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data is needed for analysis and thus should not be null\n",
    "* arrdate: needed to compare to tempurature for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count amount of records after dropping missing arrival dates\n",
    "nyc_valid = df_spark_nyc.dropna(how=\"any\", subset=[\"arrdate\"])\n",
    "num_valid_dates = nyc_valid.count()\n",
    "print(num_valid_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as it turns out there are no empty dates which is good. Now I will drop any records that don't have data for the other fields I intend to use for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_valid = nyc_valid.dropna(how=\"any\", subset=[\"I94BIR\", \"I94CIT\", \"I94RES\", \"GENDER\", \"I94MODE\"])\n",
    "num_nonnulls = nyc_valid.count()\n",
    "print(\"Number of records dropped due to missing data: \" + str(num_valid_dates - num_nonnulls))\n",
    "print(\"New Record count is: \" + str(num_nonnulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that records with missing data has been removed, all that is left for the immigration data is to get ride of any duplicates. In this case, I will assume duplicates are multiple records with the same admission number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_valid = nyc_valid.dropDuplicates([\"ADMNUM\"])\n",
    "num_nodups = nyc_valid.count()\n",
    "print(\"Number of records dropped due to duplicate data: \" + str(num_nonnulls - num_nodups))\n",
    "print(\"New Record count is: \" + str(num_nodups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the entry_city format will need to be changed so that it can match up with the other tables which refer to \"New York City\" as just \"New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_valid = nyc_valid.select(df_spark.columns)\\\n",
    "            .withColumn(\"full_entry_city\", lit(\"New York\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_valid.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will get rid of any record that don't have data in the following fields as they are essential to my analysis\n",
    "* City\n",
    "* State\n",
    "* Race\n",
    "* Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_valid = dem_df_spark.dropna(how=\"any\", subset=[\"City\", \"State\", \"Race\", \"Count\"])\n",
    "dem_nonull_count = dem_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records dropped due to missing data: \" + str(dem_count - dem_nonull_count))\n",
    "print(\"New Record count is: \" + str(dem_nonull_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, there is an additional issue with the parsing when trying to load into Redshift with parsing values, in this case any NaN values will be changed to a numeric value that clearly indicates an unknown field such as -1. This is because it would be useful to keep the field numeric but still be able to use the record. \n",
    "\n",
    "Since this project is scoped to New York, we are okay since those records have valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_valid = dem_valid.na.fill(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_valid.createOrReplaceTempView(\"dem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = spark.sql(\"\"\"\n",
    "SELECT * FROM dem\n",
    "where number_of_veterans = -1\n",
    "\"\"\")\n",
    "d.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Tempurature Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to get rid of any records with missing average tempurature data as that is the most important aspect of this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_valid = temp_df_nyc.dropna(how=\"any\", subset=[\"AverageTemperature\"])\n",
    "temp_valid_count = temp_df_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records dropped due to missing data: \" + str(temp_df_nyc.count() - temp_valid_count))\n",
    "print(\"New Record count is: \" + str(temp_valid_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport Codes Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set will need to be processed a bit differently. Because everything had to be parsed as a string, I won't be able to look for the value NaN but I can look for it as a string. Duplicates will be anything with the same identifier (ident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_na_count = air_df.filter(\"\"\"continent == 'nan' or iso_country == 'nan' \n",
    "or iso_region == 'nan' or municipality == 'nan'\n",
    "\"\"\").count()\n",
    "air_dup_count = air_df.dropDuplicates([\"ident\"]).count()\n",
    "\n",
    "print(f\"Number of records with missing data in air codes data set: {air_count - air_na_count}\")\n",
    "print(f\"Number of records with duplicate data int he air codes data set: {air_count - air_dup_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def remove_quote(field):\n",
    "    l = field.split()\n",
    "    for index, word in enumerate(l):\n",
    "        if '\\\"' in word:\n",
    "            l[index] = word.replace('\\\"',\"\")\n",
    "            print(word)\n",
    "    return \" \".join(l)\n",
    "\n",
    "spark.udf.register(\"unQuote\", remove_quote, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run udf to remove field usage of double quotes due to redshift conflict\n",
    "air_df.createOrReplaceTempView(\"air_codes\")\n",
    "air_df = spark.sql(\"\"\"\n",
    "SELECT ident,\n",
    " type,unQuote(name) as Name, elevation_ft, continent, iso_country,\n",
    " iso_region, municipality, gps_code, iata_code,\n",
    " local_code, coordinates \n",
    " FROM air_codes\n",
    "\"\"\")\n",
    "air_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nyc_valid.filter(nyc_valid.full_entry_city == \"New York\").toPandas().head()\n",
    "# dem_valid.filter(dem_valid.city == \"New York\").toPandas().head()\n",
    "# temp_df_valid.filter(temp_df_valid.City == \"New York\").toPandas().head()\n",
    "# air_df.filter(air_df.municipality == \"New York\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out there is alot of missing data in the air codes so the original data should be examined to find out why but for my purposes I will just upload the full data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write valid data frames to json\n",
    "folderpath = \"./final_csv\"\n",
    "nyc_valid.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_im_data\")\n",
    "dem_valid.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_dem_data\")\n",
    "temp_df_valid.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_temp_data\")\n",
    "air_df.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_air_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# define upload func\n",
    "# response is the bucket, should rename it\n",
    "def upload_files(path):\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                deng_bucket.put_object(Key=full_path[2:], Body=data)\n",
    "\n",
    "# upload csvs to S3\n",
    "folder_paths = [f\"{folderpath}/valid_im_data\", f\"{folderpath}/valid_dem_data\", f\"{folderpath}/valid_temp_data\",\\\n",
    "               f\"{folderpath}/valid_air_data\"]\n",
    "for data_path in folder_paths:\n",
    "    upload_files(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift Data Model Setup\n",
    "Once a redshift cluster has been provisioned, the star schema model must be set up for it as well as the staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# setting up postgres connection to redshift cluster\n",
    "conn = psycopg2.connect(f\"host={DWH_ENDPOINT} dbname={DWH_DB} user={DWH_DB_USER} password={DWH_DB_PASSWORD} port={DWH_PORT}\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up create queries\n",
    "staging_img_drop = \"DROP TABLE IF EXISTS staging_img;\"\n",
    "staging_dem_drop = \"DROP TABLE IF EXISTS staging_dem;\"\n",
    "staging_temp_drop = \"DROP TABLE IF EXISTS staging_temp;\"\n",
    "staging_air_drop = \"DROP TABLE IF EXISTS staging_air;\"\n",
    "\n",
    "immigration_drop = \"DROP TABLE IF EXISTS immigration;\"\n",
    "demographic_drop = \"DROP TABLE IF EXISTS demographics;\"\n",
    "tempurature_drop = \"DROP TABLE IF EXISTS tempuratures;\"\n",
    "airport_drop = \"DROP TABLE IF EXISTS airports;\"\n",
    "\n",
    "drop_queries = [staging_img_drop, staging_dem_drop, staging_temp_drop, staging_air_drop, immigration_drop, demographic_drop, tempurature_drop, airport_drop]\n",
    "\n",
    "staging_img_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_img(\n",
    "id bigint identity(0, 1), cicid numeric, year numeric, month numeric, city_code numeric,\n",
    "res_code numeric, entry_city varchar, arrival_date numeric, arrival_mode numeric, state varchar, \n",
    "dep_date numeric, age numeric, visa_type numeric, count numeric, dtadfile varchar, visapost varchar, \n",
    "occupation varchar, entdepa varchar, entdepd varchar, entdepu varchar, matflag varchar, birth_year numeric, \n",
    "dtaddto varchar, gender varchar, insnum varchar, airline varchar, admission_num numeric, \n",
    "fltno varchar, visa_class varchar, full_entry_city varchar);\"\"\")\n",
    "\n",
    "staging_dem_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_dem(\n",
    "city varchar, state varchar, median_age numeric, male_population numeric, female_population numeric,\n",
    "total_population bigint, number_of_veterans numeric, foreign_born numeric,\n",
    "average_household_size numeric, state_code varchar, race varchar, count bigint) \n",
    "\"\"\")\n",
    "\n",
    "staging_temp_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_temp(\n",
    "datetime varchar, average_temperature numeric, average_temperature_uncertainty numeric, city varchar,\n",
    "country varchar, latitude varchar, longitude varchar) \n",
    "\"\"\")\n",
    "\n",
    "staging_air_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_air(\n",
    "airport_id varchar, type varchar, name varchar, elevation_ft varchar, continent varchar, iso_country varchar,\n",
    "iso_region varchar, city varchar, gps_code varchar, iata_code varchar, local_code varchar,\n",
    "coordinates varchar) \n",
    "\"\"\")\n",
    "\n",
    "# create final tables ----------------\n",
    "# join entry_city on immigration to city field in the other tables\n",
    "demographic_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demographics(\n",
    "dem_id bigint identity(0, 1) PRIMARY KEY, city varchar, state varchar, median_age numeric, male_population numeric, female_population numeric,\n",
    "total_population bigint, number_of_veterans numeric, foreign_born numeric,\n",
    "average_household_size numeric, state_code varchar, race varchar, count bigint) \n",
    "\"\"\")\n",
    "\n",
    "tempurature_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tempuratures(\n",
    "temp_id bigint identity(0, 1) PRIMARY KEY, datetime varchar, average_temperature numeric, average_temperature_uncertainty numeric, city varchar,\n",
    "country varchar, latitude varchar, longitude varchar) \n",
    "\"\"\")\n",
    "\n",
    "airport_codes_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS airports(\n",
    "airport_id varchar PRIMARY KEY, type varchar, name varchar, elevation_ft varchar, continent varchar, iso_country varchar,\n",
    "iso_region varchar, city varchar, gps_code varchar, iata_code varchar, local_code varchar,\n",
    "coordinates varchar) \n",
    "\"\"\")\n",
    "\n",
    "immigration_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS immigration(\n",
    "id bigint identity(0, 1) PRIMARY KEY,  dem_id bigint, temp_id bigint, airport_id varchar, \n",
    "cicid numeric, year numeric, month numeric, city_code numeric,\n",
    "res_code numeric, entry_city varchar, arrival_date numeric, arrival_mode numeric, state varchar, \n",
    "dep_date numeric, age numeric, visa_type numeric, count numeric, dtadfile varchar, visapost varchar, \n",
    "occupation varchar, entdepa varchar, entdepd varchar, entdepu varchar, matflag varchar, birth_year numeric, \n",
    "dtaddto varchar, gender varchar, insnum varchar, airline varchar, admission_num numeric, \n",
    "fltno varchar, visa_class varchar, full_entry_city varchar,\n",
    "FOREIGN KEY(dem_id) REFERENCES demographics(dem_id),\n",
    "FOREIGN KEY(temp_id) REFERENCES tempuratures(temp_id),\n",
    "FOREIGN KEY(airport_id) REFERENCES airports(airport_id));\"\"\")\n",
    "\n",
    "create_queries = [staging_img_create, staging_dem_create, staging_temp_create, staging_air_create, demographic_create, tempurature_create, airport_codes_create, immigration_create]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create or reset database by executing drop and creat queries\n",
    "for query in drop_queries:\n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in create_queries:\n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 to Redshift Loading\n",
    "Now the data can be loaded from S3 into redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the redshift query to handle the loading\n",
    "# Need to update data sources\n",
    "staging_img_copy = (\"\"\"\n",
    "COPY staging_img FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"IM_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_dem_copy = (\"\"\"\n",
    "COPY staging_dem FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"DEM_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_temp_copy = (\"\"\"\n",
    "COPY staging_temp FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"TEMP_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_air_copy = (\"\"\"\n",
    "COPY staging_air FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"AIR_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_queries = [staging_img_copy, staging_dem_copy, staging_temp_copy, staging_air_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the redshift query\n",
    "from psycopg2 import Error\n",
    "for query in staging_queries:\n",
    "    try:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "    except Error as e:\n",
    "        print(e.diag.message_primary)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dimension table loading queries\n",
    "insert_dem = (\"\"\"\n",
    "INSERT INTO demographics(city, state, median_age, male_population, female_population,\n",
    "total_population, number_of_veterans, foreign_born,\n",
    "average_household_size, state_code, race, count)\n",
    "(\n",
    "SELECT city, state, median_age, male_population, female_population,\n",
    "total_population, number_of_veterans, foreign_born,\n",
    "average_household_size, state_code, race, count\n",
    "FROM staging_dem\n",
    ")\"\"\")\n",
    "\n",
    "insert_temp = (\"\"\"\n",
    "INSERT INTO tempuratures(datetime, average_temperature , average_temperature_uncertainty, city,\n",
    "country, latitude, longitude)\n",
    "( SELECT * FROM staging_temp )\"\"\")\n",
    "\n",
    "insert_air = (\"\"\"\n",
    "INSERT INTO airports(airport_id, type, name, elevation_ft, continent, iso_country,\n",
    "iso_region, city, gps_code, iata_code, local_code, coordinates)\n",
    "(\n",
    "SELECT airport_id, type, name, elevation_ft, continent, iso_country,\n",
    "iso_region, city, gps_code, iata_code, local_code, coordinates\n",
    "FROM staging_air\n",
    ")\"\"\")\n",
    "\n",
    "insert_im = (\"\"\"\n",
    "INSERT INTO immigration(dem_id, temp_id, airport_id, \n",
    "cicid, year, month, city_code,\n",
    "res_code, entry_city, arrival_date, arrival_mode, state, \n",
    "dep_date, age, visa_type, count, dtadfile, visapost, \n",
    "occupation, entdepa, entdepd, entdepu, matflag, birth_year, \n",
    "dtaddto, gender, insnum, airline, admission_num, fltno, visa_class, full_entry_city)\n",
    "(\n",
    "SELECT d.dem_id, t.temp_id, a.airport_id, i.cicid, i.year, i.month, i.city_code, i.res_code, i.entry_city , i.arrival_date, i.arrival_mode, i.state, \n",
    "i.dep_date, i.age, i.visa_type, i.count, i.dtadfile, i.visapost, i.occupation, i.entdepa, i.entdepd , i.entdepu , i.matflag, i.birth_year, \n",
    "i.dtaddto, i.gender, i.insnum, i.airline, i.admission_num, i.fltno, i.visa_class, i.full_entry_city\n",
    "FROM staging_img i\n",
    "LEFT JOIN demographics d ON i.entry_city = d.city\n",
    "LEFT JOIN tempuratures t ON i.entry_city = t.city\n",
    "LEFT JOIN airports a ON i.entry_city = a.city\n",
    ")\"\"\")\n",
    "\n",
    "# make sure all dim tables added\n",
    "insert_queries = [insert_air, insert_dem, insert_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute dimension table loading queries\n",
    "for query in insert_queries:\n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute fact table loading query\n",
    "query = insert_im\n",
    "cur.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM immigration\")\n",
    "im_db_count = cur.fetchone()\n",
    "print(f\"The count for the dataframe is {nyc_valid.count()} and the count for the db is {im_db_count[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM tempuratures\")\n",
    "temp_db_count = cur.fetchone()\n",
    "print(f\"The count for the dataframe is {temp_df_valid.count()} and the count for the db is {temp_db_count[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM airports\")\n",
    "air_db_count = cur.fetchone()\n",
    "print(f\"The count for the dataframe is {air_df.count()} and the count for the db is {air_db_count[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM demographics\")\n",
    "dem_db_count = cur.fetchone()\n",
    "print(f\"The count for the dataframe is {dem_valid.count()} and the count for the db is {dem_db_count[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM immigration LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM demographics LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM tempuratures LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM airports LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: dates were left as SAS numberic dates to allow flexibility in parsing depending on where data is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT to_date('19600101', 'YYYYMMDD') + cast(arrival_date as integer) AS \"Date\", COUNT(*) FROM immigration group by arrival_date LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "SELECT to_date('19600101', 'YYYYMMDD') + cast(arrival_date as integer) AS \"Date\", COUNT(*) FROM immigration group by arrival_date order by \"Date\" asc\n",
    "\"\"\")\n",
    "res_df = pd.DataFrame(cur.fetchall())\n",
    "res_df.columns = [\"date\", \"count\"]\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(12, 6))\n",
    "plt.plot(res_df[\"date\"], res_df[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dictionaries are in the data_dictionary folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary tools used in this project are Spark, Pandas, S3 for the data lake and Redshift for the dimensional model. With respect to data processing and exploration, Spark is a key tool as it provides the parallelized abstraction of the Spark data frame and operates on that parallelized data structure in memory, making the ETL process much faster than traditional data processing tools such as MapReduce. When it comes to visualizing the data, Pandas is used because it integrates well with jupyter notebooks making the tables look much cleaner than simple df.show() on a spark data frame. S3 is used as the data lake because it provides an easy to use interface for storing lots of files and the cost is relativley low. On top of this, it has good integration with Redshift and allows the loading to be done in parallel just like spark allowed for the transformations. Redshift is used because as a column based database that can be interfaced with using postgres, it provides both the query speed that a data ware house needs and the familiarity of PostgreSQL, allowing existing SQL based workloads such as those an analyst might use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often the data should be updated depends on the use for this pipeline. The scope of this project prepared a data set for examining possible relationships between tempurature and immigration. If this information is to be used for long term urban planning projects, less frequent updates, such as daily throughout the project's life, would suffice. If however, the data set will be used for operational decision that need to be made faster, such as new immigration decisions, it must be updated much more frequently such as hourly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data was increased 100x, I would have to be much more careful about operating on it and tracking changes. In all liklihood I would load the data set into S3 first to free up on prem space requirements. I would also increase the amount of nodes in the redshift cluster to handle the extra data loading.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, I went through this project keeping in mind that someone will need to use to it. This is why I chose to use Redshift. Since it can interfaced with using PostgreSQL, pretty much any analyst or BI specialist - of which SQL knowledge is a basic requirement - can interact with the data. In addition, because of the popularity of PostgreSQL, most popular dashboard tools like Tableau, Power BI etc. will likely have native, easy to use support for things like connectors to PostgreSQL databases. The fact that it would need to be updated at a specific time each day would make an scheduling tool like airflow more necessary and would handle running all of the code in this notebook before the dashboard needs to be examined each day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data needed to be accessed by 100+ people, Redshift configuration can be changed to make it more available. Some examples of that would be having Redshift servicing multiple availability zones. In addition, if there are common workloads or queries that need to be run by many of these people, they can be prepared in advanced such as pre aggregated OLAP cubes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
