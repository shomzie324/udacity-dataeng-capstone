{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from pyspark.sql.functions import *\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "This project will focus on exploring the relationship between tempurature and immigration in a specific state such as New York or city such as new york city. This will require using only the portions of the US Immigration Data in which the arrival was in a city in new york. To supplement this data, US tempurature data will also be used. In addition, demographic data will be added to explore relationships if any between tempurature and the frequency of immigration within a specifc demographic.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in immigration data\t\n",
    "# note: too large to read everything into memory at once\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline', 'admnum', 'fltno', 'visatype']\n"
     ]
    }
   ],
   "source": [
    "# check schema of df \n",
    "print(df_spark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U     None   1979.0  10282016   None   None   \n",
       "1      NaN   ...           Y     None   1991.0       D/S      M   None   \n",
       "2  20691.0   ...        None        M   1961.0  09302016      M   None   \n",
       "3  20567.0   ...        None        M   1988.0  09302016   None   None   \n",
       "4  20567.0   ...        None        M   2012.0  09302016   None   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0    None  1.897628e+09   None       B2  \n",
       "1    None  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_pd = pd.DataFrame(df_spark.take(5))\n",
    "im_pd.columns = df_spark.columns\n",
    "im_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read demographic data\n",
    "dem_df = pd.read_csv(\"us-cities-demographics.csv\", sep=\";\")\n",
    "dem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'State', 'Median Age', 'Male Population', 'Female Population',\n",
       "       'Total Population', 'Number of Veterans', 'Foreign-born',\n",
       "       'Average Household Size', 'State Code', 'Race', 'Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Tempurature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in tempurature data\n",
    "temp_df = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'City',\n",
       "       'Country', 'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airport Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read airport codes\n",
    "air_codes_df = pd.read_csv(\"airport-codes_csv.csv\")\n",
    "air_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ident', 'type', 'name', 'elevation_ft', 'continent', 'iso_country',\n",
       "       'iso_region', 'municipality', 'gps_code', 'iata_code', 'local_code',\n",
       "       'coordinates'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_codes_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data\n",
    "* Identify data quality issues, like missing values, duplicate data, etc.\n",
    "* Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Immigration Data Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume\n",
    "Since the data analysis is focused on the relationship between tempurature and immigration in new york city , the exploration will be focused on that subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485916"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_nyc = df_spark.filter(df_spark.i94port == \"NYC\")\n",
    "df_spark_nyc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets take a look at the columns again just for the new york city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20555.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247104e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>20558.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247140e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>20558.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247161e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "1   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "2   18.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MI   \n",
       "3   19.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NJ   \n",
       "4   20.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NJ   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0  20567.0   ...        None        M   1988.0  09302016   None   None   \n",
       "1  20567.0   ...        None        M   2012.0  09302016   None   None   \n",
       "2  20555.0   ...        None        M   1959.0  09302016   None   None   \n",
       "3  20558.0   ...        None        M   1953.0  09302016   None   None   \n",
       "4  20558.0   ...        None        M   1959.0  09302016   None   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0      AA  9.246846e+10  00199       B2  \n",
       "1      AA  9.246846e+10  00199       B2  \n",
       "2      AZ  9.247104e+10  00602       B1  \n",
       "3      AZ  9.247140e+10  00602       B2  \n",
       "4      AZ  9.247161e+10  00602       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_nyc.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Demographic Data Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2891\n"
     ]
    }
   ],
   "source": [
    "# get new york city data and check volume\n",
    "# dem_df_nyc = dem_df[dem_df['City'] == \"New York\"]\n",
    "# dem_df_nyc.count()\n",
    "dem_df.columns = ['city', 'state', 'median_age', 'male_population','female_population',\n",
    "                     'total_population', 'number_of_veterans','foreign_born', 'average_household_size',\n",
    "                     'state_code','race','count']\n",
    "dem_df_spark = spark.createDataFrame(dem_df)\n",
    "dem_count = dem_df_spark.count()\n",
    "print(dem_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, I will take a look at the demographic city data to explore what data is there and if there are any issues with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>race</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city          state  median_age  male_population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   female_population  total_population  number_of_veterans  foreign_born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   average_household_size state_code                       race  count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_df_spark.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this project, I scoped my analysis to a single city so I will only need the few records that pertain to New York, New York specifically, but I will still assess the data as whole as it pertains to duplicates and missing fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with missing data in demographics data set: 16\n",
      "Number of records with duplicate data in demographics data set: 0\n"
     ]
    }
   ],
   "source": [
    "dem_na_count = dem_df_spark.dropna(how=\"any\").count()\n",
    "dem_dup_count = dem_df_spark.dropDuplicates().count()\n",
    "\n",
    "print(f\"Number of records with missing data in demographics data set: {dem_count - dem_na_count}\")\n",
    "print(f\"Number of records with duplicate data in demographics data set: {dem_count - dem_dup_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Tempurature Data Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nyc sample\n",
    "temp_df_nyc = spark.createDataFrame(temp_df[temp_df['City'] == \"New York\"])\n",
    "temp_count = temp_df_nyc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>3.264</td>\n",
       "      <td>1.665</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty      City  \\\n",
       "0  1743-11-01               3.264                          1.665  New York   \n",
       "1  1743-12-01                 NaN                            NaN  New York   \n",
       "2  1744-01-01                 NaN                            NaN  New York   \n",
       "3  1744-02-01                 NaN                            NaN  New York   \n",
       "4  1744-03-01                 NaN                            NaN  New York   \n",
       "\n",
       "         Country Latitude Longitude  \n",
       "0  United States   40.99N    74.56W  \n",
       "1  United States   40.99N    74.56W  \n",
       "2  United States   40.99N    74.56W  \n",
       "3  United States   40.99N    74.56W  \n",
       "4  United States   40.99N    74.56W  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read tempurature sample\n",
    "temp_df_nyc.toPandas().head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will assess how many records are duplicates or have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with missing data in demographics data set: 120\n",
      "Number of records with duplicate data in demographics data set: 0\n"
     ]
    }
   ],
   "source": [
    "temp_na_count = temp_df_nyc.dropna(how=\"any\").count()\n",
    "temp_dup_count = temp_df_nyc.dropDuplicates().count()\n",
    "\n",
    "print(f\"Number of records with missing data in demographics data set: {temp_count - temp_na_count}\")\n",
    "print(f\"Number of records with duplicate data in demographics data set: {temp_count - temp_dup_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airport Code Explortion & Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though airport codes are not needed for the scope of this project, it will be processed anyways in case future analysis requires it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before the data can be used with the spark, thedata types need to be turned into something spark can parse, as currently everything was read in as object, for now everything will be made to be a string and adjusted to their proper type in the cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ident', 'type', 'name', 'elevation_ft', 'continent', 'iso_country',\n",
      "       'iso_region', 'municipality', 'gps_code', 'iata_code', 'local_code',\n",
      "       'coordinates'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(air_codes_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_codes_df = air_codes_df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident           object\n",
       "type            object\n",
       "name            object\n",
       "elevation_ft    object\n",
       "continent       object\n",
       "iso_country     object\n",
       "iso_region      object\n",
       "municipality    object\n",
       "gps_code        object\n",
       "iata_code       object\n",
       "local_code      object\n",
       "coordinates     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_codes_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count air code data\n",
    "air_df = spark.createDataFrame(air_codes_df)\n",
    "air_count = air_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55075\n"
     ]
    }
   ],
   "source": [
    "print(air_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first I will look at the data just to remind us of what is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>nan</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>nan</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>nan</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>nan</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport         11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport       3435.0   \n",
       "2  00AK  small_airport                        Lowell Field        450.0   \n",
       "3  00AL  small_airport                        Epps Airpark        820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport        237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       nan          US      US-PA      Bensalem      00A       nan   \n",
       "1       nan          US      US-KS         Leoti     00AA       nan   \n",
       "2       nan          US      US-AK  Anchor Point     00AK       nan   \n",
       "3       nan          US      US-AL       Harvest     00AL       nan   \n",
       "4       nan          US      US-AR       Newport      nan       nan   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        nan                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the Data Model\n",
    "## 3.1 Conceptual Data Model\n",
    "(Map out the conceptual data model and explain why you chose that model)\n",
    "\n",
    "The Data Model that will be used is a star schema. This is because since there will be so much data in the data warehouse, it is important to simplify the joins as not doing so will significantly slow down queries. \n",
    "\n",
    "The us immigration data will serve as the fact table since that is the primary event in question i.e people immigrating into the US. The demographic data will be a dimension table and should join with the immigration data via the city name. The Tempurature data will be another dimension table which can join with the immigration data based on the date of arrival. The airport codes can be joined based on the city name.\n",
    "\n",
    "## 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "Currently the data exists as csv files and the end goal is to have a datawarehouse that can be quried. AWS Will be used for the pipeline and the steps are as follows:\n",
    "\n",
    "1. extract the source data csv files using spark and examine and report and the data quality\n",
    "2. clean the data as per the above cleaning transformation requirements\n",
    "3. set up AWS using IAC and create the needed S3 buckets for the data lake and the tables needed for redshift\n",
    "4. load the cleaned spark dataframes into S3 as csv files\n",
    "5. use S3 to load data into redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "## 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the confugration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwhCluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwhRole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param       Value\n",
       "0        DWH_CLUSTER_TYPE  multi-node\n",
       "1           DWH_NUM_NODES           4\n",
       "2           DWH_NODE_TYPE   dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER  dwhCluster\n",
       "4                  DWH_DB         dwh\n",
       "5             DWH_DB_USER     dwhuser\n",
       "6         DWH_DB_PASSWORD    Passw0rd\n",
       "7                DWH_PORT        5439\n",
       "8       DWH_IAM_ROLE_NAME     dwhRole"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up clients for EC2, S3, IAM and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use S3 client to create a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket\n",
    "try:\n",
    "    location = {'LocationConstraint': \"us-west-2\"}\n",
    "    s3.create_bucket(Bucket=\"deng-capstone\", \\\n",
    "    CreateBucketConfiguration=location)\n",
    "except ClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List buckets to ensure it was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deng_bucket = s3.Bucket(\"deng-capstone\")\n",
    "\n",
    "# Output the bucket object names\n",
    "for obj in deng_bucket.objects.filter():\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IAM Role for redshift to S3 access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 Attaching Policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach a policy\n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::243965846266:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get and print the IAM role ARN to make sure it worked\n",
    "print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create redshift cluster with provided configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display cluster info to see when it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.ch7ro1wfowik.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-215ef659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "7  NumberOfNodes       \n",
       "\n",
       "                                                                                   Value  \n",
       "0  dwhcluster                                                                             \n",
       "1  dc2.large                                                                              \n",
       "2  available                                                                              \n",
       "3  dwhuser                                                                                \n",
       "4  dwh                                                                                    \n",
       "5  {'Address': 'dwhcluster.ch7ro1wfowik.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-215ef659                                                                           \n",
       "7  4                                                                                      "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "# get 1st cluster from cluster list (only 1 in this case)\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When cluster is ready, get the info needed to connect to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.ch7ro1wfowik.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::243965846266:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "### Wait Until Redshift is available before running this\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open an incoming  TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-62233128')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connection to redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.ch7ro1wfowik.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When ready, delete cluster (Do not do this until project run through is complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'dwhcluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'MasterUsername': 'dwhuser',\n",
       "  'DBName': 'dwh',\n",
       "  'Endpoint': {'Address': 'dwhcluster.ch7ro1wfowik.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2019, 9, 18, 14, 51, 18, 977000, tzinfo=tzlocal()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-62233128',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-215ef659',\n",
       "  'AvailabilityZone': 'us-west-2c',\n",
       "  'PreferredMaintenanceWindow': 'tue:11:30-tue:12:00',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 4,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::243965846266:role/dwhRole',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current'},\n",
       " 'ResponseMetadata': {'RequestId': '3b6c29e1-da4a-11e9-ab0e-4d85d7c1dfce',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3b6c29e1-da4a-11e9-ab0e-4d85d7c1dfce',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2290',\n",
       "   'vary': 'Accept-Encoding',\n",
       "   'date': 'Wed, 18 Sep 2019 19:26:37 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction\n",
    "Since the data has already been retreived so that it could be explored and assessed in a previous step the extraction step is already handled. All that is left is to transform and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data is needed for analysis and thus should not be null\n",
    "* arrdate: needed to compare to tempurature for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485916\n"
     ]
    }
   ],
   "source": [
    "# count amount of records after dropping missing arrival dates\n",
    "nyc_valid = df_spark_nyc.dropna(how=\"any\", subset=[\"arrdate\"])\n",
    "num_valid_dates = nyc_valid.count()\n",
    "print(num_valid_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as it turns out there are no empty dates which is good. Now I will drop any records that don't have data for the other fields I intend to use for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records dropped due to missing data: 88079\n",
      "New Record count is: 397837\n"
     ]
    }
   ],
   "source": [
    "nyc_valid = nyc_valid.dropna(how=\"any\", subset=[\"I94BIR\", \"I94CIT\", \"I94RES\", \"GENDER\", \"I94MODE\"])\n",
    "num_nonnulls = nyc_valid.count()\n",
    "print(\"Number of records dropped due to missing data: \" + str(num_valid_dates - num_nonnulls))\n",
    "print(\"New Record count is: \" + str(num_nonnulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that records with missing data has been removed, all that is left for the immigration data is to get ride of any duplicates. In this case, I will assume duplicates are multiple records with the same admission number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records dropped due to duplicate data: 78\n",
      "New Record count is: 397759\n"
     ]
    }
   ],
   "source": [
    "nyc_valid = nyc_valid.dropDuplicates([\"ADMNUM\"])\n",
    "num_nodups = nyc_valid.count()\n",
    "print(\"Number of records dropped due to duplicate data: \" + str(num_nonnulls - num_nodups))\n",
    "print(\"New Record count is: \" + str(num_nodups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the entry_city format will need to be changed so that it can match up with the other tables with refer to \"New York City\" as just \"New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_valid = nyc_valid.select(df_spark.columns)\\\n",
    "            .withColumn(\"full_entry_city\", lit(\"New York\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "      <th>full_entry_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58192.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20549.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NH</td>\n",
       "      <td>665926385.0</td>\n",
       "      <td>10</td>\n",
       "      <td>WT</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19734.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>LH</td>\n",
       "      <td>666678085.0</td>\n",
       "      <td>410</td>\n",
       "      <td>WT</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21953.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20634.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>IB</td>\n",
       "      <td>666764185.0</td>\n",
       "      <td>6251</td>\n",
       "      <td>WT</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>431293.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20634.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>09292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>S3</td>\n",
       "      <td>671104685.0</td>\n",
       "      <td>1521</td>\n",
       "      <td>B2</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>492790.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20547.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>10022016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>QR</td>\n",
       "      <td>673426585.0</td>\n",
       "      <td>703</td>\n",
       "      <td>B1</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0  58192.0   2016.0  4.0     209.0   209.0   NYC     20545.0  1.0      None     \n",
       "1  19734.0   2016.0  4.0     124.0   124.0   NYC     20545.0  1.0      NY       \n",
       "2  21953.0   2016.0  4.0     129.0   129.0   NYC     20545.0  1.0      NY       \n",
       "3  431293.0  2016.0  4.0     696.0   696.0   NYC     20546.0  1.0      FL       \n",
       "4  492790.0  2016.0  4.0     213.0   213.0   NYC     20547.0  1.0      NY       \n",
       "\n",
       "   depdate       ...        matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0  20549.0       ...        M        1976.0   06292016  M      None   NH       \n",
       "1  20561.0       ...        M        1996.0   06292016  F      None   LH       \n",
       "2  20634.0       ...        M        1954.0   06292016  F      None   IB       \n",
       "3  20634.0       ...        M        1968.0   09292016  M      None   S3       \n",
       "4  20553.0       ...        M        1952.0   10022016  M      None   QR       \n",
       "\n",
       "        admnum fltno visatype full_entry_city  \n",
       "0  665926385.0  10    WT       New York        \n",
       "1  666678085.0  410   WT       New York        \n",
       "2  666764185.0  6251  WT       New York        \n",
       "3  671104685.0  1521  B2       New York        \n",
       "4  673426585.0  703   B1       New York        \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_valid.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will get rid of any record that don't have data in the following fields as they are essential to my analysis\n",
    "* City\n",
    "* State\n",
    "* Race\n",
    "* Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_valid = dem_df_spark.dropna(how=\"any\", subset=[\"City\", \"State\", \"Race\", \"Count\"])\n",
    "dem_nonull_count = dem_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records dropped due to missing data: 0\n",
      "New Record count is: 2891\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records dropped due to missing data: \" + str(dem_count - dem_nonull_count))\n",
    "print(\"New Record count is: \" + str(dem_nonull_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, there is an additional issue with the parsing when trying to load into Redshift with parsing values, in this case any NaN values will be changed to a numeric value that clearly indicates an unknown field such as -1. This is because it would be useful to keep the field numeric but still be able to use the record. \n",
    "\n",
    "Since this project is scoped to New York, we are okay since those records have valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_valid = dem_valid.na.fill(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_valid.createOrReplaceTempView(\"dem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>race</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>335559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caguas</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>40.4</td>\n",
       "      <td>34743.0</td>\n",
       "      <td>42265.0</td>\n",
       "      <td>77008</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>76349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>12143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carolina</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64758.0</td>\n",
       "      <td>77308.0</td>\n",
       "      <td>142066</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>PR</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>139967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>San Juan</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>41.4</td>\n",
       "      <td>155408.0</td>\n",
       "      <td>186829.0</td>\n",
       "      <td>342237</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>PR</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>4031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city        state  median_age  male_population  female_population  \\\n",
       "0  San Juan  Puerto Rico  41.4        155408.0         186829.0            \n",
       "1  Caguas    Puerto Rico  40.4        34743.0          42265.0             \n",
       "2  Carolina  Puerto Rico  42.0        64758.0          77308.0             \n",
       "3  Carolina  Puerto Rico  42.0        64758.0          77308.0             \n",
       "4  San Juan  Puerto Rico  41.4        155408.0         186829.0            \n",
       "\n",
       "   total_population  number_of_veterans  foreign_born  average_household_size  \\\n",
       "0  342237           -1.0                -1.0          -1.0                      \n",
       "1  77008            -1.0                -1.0          -1.0                      \n",
       "2  142066           -1.0                -1.0          -1.0                      \n",
       "3  142066           -1.0                -1.0          -1.0                      \n",
       "4  342237           -1.0                -1.0          -1.0                      \n",
       "\n",
       "  state_code                               race   count  \n",
       "0  PR         Hispanic or Latino                 335559  \n",
       "1  PR         Hispanic or Latino                 76349   \n",
       "2  PR         American Indian and Alaska Native  12143   \n",
       "3  PR         Hispanic or Latino                 139967  \n",
       "4  PR         American Indian and Alaska Native  4031    "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = spark.sql(\"\"\"\n",
    "SELECT * FROM dem\n",
    "where number_of_veterans = -1\n",
    "\"\"\")\n",
    "d.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Tempurature Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to get rid of any records with missing average tempurature data as that is the most important aspect of this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_valid = temp_df_nyc.dropna(how=\"any\", subset=[\"AverageTemperature\"])\n",
    "temp_valid_count = temp_df_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records dropped due to missing data: 120\n",
      "New Record count is: 3119\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records dropped due to missing data: \" + str(temp_df_nyc.count() - temp_valid_count))\n",
    "print(\"New Record count is: \" + str(temp_valid_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport Codes Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set will need to be processed a bit differently. Because everything had to be parsed as a string, I won't be able to look for the value NaN but I can look for it as a string. Duplicates will be anything with the same identifier (ident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with missing data in air codes data set: 22499\n",
      "Number of records with duplicate data int he air codes data set: 0\n"
     ]
    }
   ],
   "source": [
    "air_na_count = air_df.filter(\"\"\"continent == 'nan' or iso_country == 'nan' \n",
    "or iso_region == 'nan' or municipality == 'nan'\n",
    "\"\"\").count()\n",
    "air_dup_count = air_df.dropDuplicates([\"ident\"]).count()\n",
    "\n",
    "print(f\"Number of records with missing data in air codes data set: {air_count - air_na_count}\")\n",
    "print(f\"Number of records with duplicate data int he air codes data set: {air_count - air_dup_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_quote(field)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def remove_quote(field):\n",
    "    l = field.split()\n",
    "    for index, word in enumerate(l):\n",
    "        if '\\\"' in word:\n",
    "            l[index] = word.replace('\\\"',\"\")\n",
    "            print(word)\n",
    "    return \" \".join(l)\n",
    "\n",
    "spark.udf.register(\"unQuote\", remove_quote, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>Name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>nan</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>nan</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>nan</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>nan</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                Name elevation_ft  \\\n",
       "0  00A   heliport       Total Rf Heliport                   11.0          \n",
       "1  00AA  small_airport  Aero B Ranch Airport                3435.0        \n",
       "2  00AK  small_airport  Lowell Field                        450.0         \n",
       "3  00AL  small_airport  Epps Airpark                        820.0         \n",
       "4  00AR  closed         Newport Hospital & Clinic Heliport  237.0         \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0  nan       US          US-PA      Bensalem      00A      nan        \n",
       "1  nan       US          US-KS      Leoti         00AA     nan        \n",
       "2  nan       US          US-AK      Anchor Point  00AK     nan        \n",
       "3  nan       US          US-AL      Harvest       00AL     nan        \n",
       "4  nan       US          US-AR      Newport       nan      nan        \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0  00A        -74.93360137939453, 40.07080078125     \n",
       "1  00AA       -101.473911, 38.704022                 \n",
       "2  00AK       -151.695999146, 59.94919968            \n",
       "3  00AL       -86.77030181884766, 34.86479949951172  \n",
       "4  nan        -91.254898, 35.6087                    "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run udf to remove field usage of double quotes due to redshift conflict\n",
    "air_df.createOrReplaceTempView(\"air_codes\")\n",
    "air_df = spark.sql(\"\"\"\n",
    "SELECT ident,\n",
    " type,unQuote(name) as Name, elevation_ft, continent, iso_country,\n",
    " iso_region, municipality, gps_code, iata_code,\n",
    " local_code, coordinates \n",
    " FROM air_codes\n",
    "\"\"\")\n",
    "air_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>Name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6N5</td>\n",
       "      <td>heliport</td>\n",
       "      <td>East 34th Street Heliport</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>6N5</td>\n",
       "      <td>TSS</td>\n",
       "      <td>6N5</td>\n",
       "      <td>-73.97209930419922, 40.74259948730469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6N6</td>\n",
       "      <td>seaplane_base</td>\n",
       "      <td>Evers Seaplane Base</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>6N6</td>\n",
       "      <td>nan</td>\n",
       "      <td>6N6</td>\n",
       "      <td>-73.81620025634766, 40.84590148925781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6N7</td>\n",
       "      <td>seaplane_base</td>\n",
       "      <td>New York Skyports Inc Seaplane Base</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>nan</td>\n",
       "      <td>QNY</td>\n",
       "      <td>6N7</td>\n",
       "      <td>-73.9729, 40.734001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JPB</td>\n",
       "      <td>closed</td>\n",
       "      <td>Pan Am Building Heliport</td>\n",
       "      <td>870.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>nan</td>\n",
       "      <td>JPB</td>\n",
       "      <td>nan</td>\n",
       "      <td>-73.9765, 40.7533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JRA</td>\n",
       "      <td>heliport</td>\n",
       "      <td>West 30th St. Heliport</td>\n",
       "      <td>7.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>US</td>\n",
       "      <td>US-NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>KJRA</td>\n",
       "      <td>JRA</td>\n",
       "      <td>JRA</td>\n",
       "      <td>-74.007103, 40.754501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                 Name elevation_ft  \\\n",
       "0  6N5   heliport       East 34th Street Heliport            10.0          \n",
       "1  6N6   seaplane_base  Evers Seaplane Base                  nan           \n",
       "2  6N7   seaplane_base  New York Skyports Inc Seaplane Base  nan           \n",
       "3  JPB   closed         Pan Am Building Heliport             870.0         \n",
       "4  JRA   heliport       West 30th St. Heliport               7.0           \n",
       "\n",
       "  continent iso_country iso_region municipality gps_code iata_code local_code  \\\n",
       "0  nan       US          US-NY      New York     6N5      TSS       6N5         \n",
       "1  nan       US          US-NY      New York     6N6      nan       6N6         \n",
       "2  nan       US          US-NY      New York     nan      QNY       6N7         \n",
       "3  nan       US          US-NY      New York     nan      JPB       nan         \n",
       "4  nan       US          US-NY      New York     KJRA     JRA       JRA         \n",
       "\n",
       "                             coordinates  \n",
       "0  -73.97209930419922, 40.74259948730469  \n",
       "1  -73.81620025634766, 40.84590148925781  \n",
       "2  -73.9729, 40.734001                    \n",
       "3  -73.9765, 40.7533                      \n",
       "4  -74.007103, 40.754501                  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nyc_valid.filter(nyc_valid.full_entry_city == \"New York\").toPandas().head()\n",
    "# dem_valid.filter(dem_valid.city == \"New York\").toPandas().head()\n",
    "# temp_df_valid.filter(temp_df_valid.City == \"New York\").toPandas().head()\n",
    "# air_df.filter(air_df.municipality == \"New York\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out there is alot of missing data in the air codes so the original data should be examined to find out why but for my purposes I will just upload the full data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write valid data frames to json\n",
    "folderpath = \"./final_csv\"\n",
    "nyc_valid.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_im_data\")\n",
    "dem_valid.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_dem_data\")\n",
    "temp_df_valid.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_temp_data\")\n",
    "air_df.write.format(\"csv\").option(\"header\", \"False\").save(f\"{folderpath}/valid_air_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# define upload func\n",
    "# response is the bucket, should rename it\n",
    "def upload_files(path):\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            print(full_path)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                deng_bucket.put_object(Key=full_path[2:], Body=data)\n",
    "\n",
    "# upload csvs to S3\n",
    "folder_paths = [f\"{folderpath}/valid_im_data\", f\"{folderpath}/valid_dem_data\", f\"{folderpath}/valid_temp_data\",\\\n",
    "               f\"{folderpath}/valid_air_data\"]\n",
    "for data_path in folder_paths:\n",
    "    upload_files(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift Data Model Setup\n",
    "Once a redshift cluster has been provisioned, the star schema model must be set up for it as well as the staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# setting up postgres connection to redshift cluster\n",
    "conn = psycopg2.connect(f\"host={DWH_ENDPOINT} dbname={DWH_DB} user={DWH_DB_USER} password={DWH_DB_PASSWORD} port={DWH_PORT}\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up create queries\n",
    "staging_img_drop = \"DROP TABLE IF EXISTS staging_img;\"\n",
    "staging_dem_drop = \"DROP TABLE IF EXISTS staging_dem;\"\n",
    "staging_temp_drop = \"DROP TABLE IF EXISTS staging_temp;\"\n",
    "staging_air_drop = \"DROP TABLE IF EXISTS staging_air;\"\n",
    "\n",
    "immigration_drop = \"DROP TABLE IF EXISTS immigration;\"\n",
    "demographic_drop = \"DROP TABLE IF EXISTS demographics;\"\n",
    "tempurature_drop = \"DROP TABLE IF EXISTS tempuratures;\"\n",
    "airport_drop = \"DROP TABLE IF EXISTS airports;\"\n",
    "\n",
    "drop_queries = [staging_img_drop, staging_dem_drop, staging_temp_drop, staging_air_drop, immigration_drop, demographic_drop, tempurature_drop, airport_drop]\n",
    "\n",
    "staging_img_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_img(\n",
    "id bigint identity(0, 1), cicid numeric, year numeric, month numeric, city_code numeric,\n",
    "res_code numeric, entry_city varchar, arrival_date numeric, arrival_mode numeric, state varchar, \n",
    "dep_date numeric, age numeric, visa_type numeric, count numeric, dtadfile varchar, visapost varchar, \n",
    "occupation varchar, entdepa varchar, entdepd varchar, entdepu varchar, matflag varchar, birth_year numeric, \n",
    "dtaddto varchar, gender varchar, insnum varchar, airline varchar, admission_num numeric, \n",
    "fltno varchar, visatype varchar, full_entry_city varchar);\"\"\")\n",
    "\n",
    "staging_dem_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_dem(\n",
    "city varchar, state varchar, median_age numeric, male_population numeric, female_population numeric,\n",
    "total_population bigint, number_of_veterans numeric, foreign_born numeric,\n",
    "average_household_size numeric, state_code varchar, race varchar, count bigint) \n",
    "\"\"\")\n",
    "\n",
    "staging_temp_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_temp(\n",
    "datetime varchar, average_temperature numeric, average_temperature_uncertainty numeric, city varchar,\n",
    "country varchar, latitude varchar, longitude varchar) \n",
    "\"\"\")\n",
    "\n",
    "staging_air_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_air(\n",
    "airport_id varchar, type varchar, name varchar, elevation_ft varchar, continent varchar, iso_country varchar,\n",
    "iso_region varchar, city varchar, gps_code varchar, iata_code varchar, local_code varchar,\n",
    "coordinates varchar) \n",
    "\"\"\")\n",
    "\n",
    "# create final tables ----------------\n",
    "# join entry_city on immigration to city field in the other tables\n",
    "demographic_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demographics(\n",
    "dem_id bigint identity(0, 1) PRIMARY KEY, city varchar, state varchar, median_age numeric, male_population numeric, female_population numeric,\n",
    "total_population bigint, number_of_veterans numeric, foreign_born numeric,\n",
    "average_household_size numeric, state_code varchar, race varchar, count bigint) \n",
    "\"\"\")\n",
    "\n",
    "tempurature_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tempuratures(\n",
    "temp_id bigint identity(0, 1) PRIMARY KEY, datetime varchar, average_temperature numeric, average_temperature_uncertainty numeric, city varchar,\n",
    "country varchar, latitude varchar, longitude varchar) \n",
    "\"\"\")\n",
    "\n",
    "airport_codes_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS airports(\n",
    "airport_id varchar PRIMARY KEY, type varchar, name varchar, elevation_ft varchar, continent varchar, iso_country varchar,\n",
    "iso_region varchar, city varchar, gps_code varchar, iata_code varchar, local_code varchar,\n",
    "coordinates varchar) \n",
    "\"\"\")\n",
    "\n",
    "immigration_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS immigration(\n",
    "id bigint identity(0, 1) PRIMARY KEY,  dem_id bigint, temp_id bigint, airport_id varchar, \n",
    "cicid numeric, year numeric, month numeric, city_code numeric,\n",
    "res_code numeric, entry_city varchar, arrival_date numeric, arrival_mode numeric, state varchar, \n",
    "dep_date numeric, age numeric, visa_type numeric, count numeric, dtadfile varchar, visapost varchar, \n",
    "occupation varchar, entdepa varchar, entdepd varchar, entdepu varchar, matflag varchar, birth_year numeric, \n",
    "dtaddto varchar, gender varchar, insnum varchar, airline varchar, admission_num numeric, \n",
    "fltno varchar, visatype varchar, full_entry_city varchar,\n",
    "FOREIGN KEY(dem_id) REFERENCES demographics(dem_id),\n",
    "FOREIGN KEY(temp_id) REFERENCES tempuratures(temp_id),\n",
    "FOREIGN KEY(airport_id) REFERENCES airports(airport_id));\"\"\")\n",
    "\n",
    "create_queries = [staging_img_create, staging_dem_create, staging_temp_create, staging_air_create, demographic_create, tempurature_create, airport_codes_create, immigration_create]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS staging_img;\n",
      "DROP TABLE IF EXISTS staging_dem;\n",
      "DROP TABLE IF EXISTS staging_temp;\n",
      "DROP TABLE IF EXISTS staging_air;\n",
      "DROP TABLE IF EXISTS immigration;\n",
      "DROP TABLE IF EXISTS demographics;\n",
      "DROP TABLE IF EXISTS tempuratures;\n",
      "DROP TABLE IF EXISTS airports;\n"
     ]
    }
   ],
   "source": [
    "# create or reset database by executing drop and creat queries\n",
    "for query in drop_queries:\n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS immigration;\n",
      "\n",
      "CREATE TABLE IF NOT EXISTS immigration(\n",
      "id bigint identity(0, 1) PRIMARY KEY,  dem_id bigint, temp_id bigint, airport_id varchar, \n",
      "cicid numeric, year numeric, month numeric, city_code numeric,\n",
      "res_code numeric, entry_city varchar, arrival_date numeric, arrival_mode numeric, state varchar, \n",
      "dep_date numeric, age numeric, visa_type numeric, count numeric, dtadfile varchar, visapost varchar, \n",
      "occupation varchar, entdepa varchar, entdepd varchar, entdepu varchar, matflag varchar, birth_year numeric, \n",
      "dtaddto varchar, gender varchar, insnum varchar, airline varchar, admission_num numeric, \n",
      "fltno varchar, visatype varchar, full_entry_city varchar,\n",
      "FOREIGN KEY(dem_id) REFERENCES demographics(dem_id),\n",
      "FOREIGN KEY(temp_id) REFERENCES tempuratures(temp_id),\n",
      "FOREIGN KEY(airport_id) REFERENCES airports(airport_id));\n"
     ]
    }
   ],
   "source": [
    "for query in create_queries:\n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 to Redshift Loading\n",
    "Now the data can be loaded from S3 into redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the redshift query to handle the loading\n",
    "# Need to update data sources\n",
    "staging_img_copy = (\"\"\"\n",
    "COPY staging_img FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"IM_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_dem_copy = (\"\"\"\n",
    "COPY staging_dem FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"DEM_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_temp_copy = (\"\"\"\n",
    "COPY staging_temp FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"TEMP_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_air_copy = (\"\"\"\n",
    "COPY staging_air FROM '{}/part' \n",
    "CREDENTIALS 'aws_iam_role={}'\n",
    "csv REGION 'us-west-2'\n",
    "delimiter ',';\n",
    "\"\"\").format(config.get(\"S3\", \"AIR_DATA\"), DWH_ROLE_ARN)\n",
    "\n",
    "staging_queries = [staging_img_copy, staging_dem_copy, staging_temp_copy, staging_air_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COPY staging_img FROM 's3://deng-capstone/final_csv/valid_im_data/part' \n",
      "CREDENTIALS 'aws_iam_role=arn:aws:iam::243965846266:role/dwhRole'\n",
      "csv REGION 'us-west-2'\n",
      "delimiter ',';\n",
      "\n",
      "\n",
      "COPY staging_dem FROM 's3://deng-capstone/final_csv/valid_dem_data/part' \n",
      "CREDENTIALS 'aws_iam_role=arn:aws:iam::243965846266:role/dwhRole'\n",
      "csv REGION 'us-west-2'\n",
      "delimiter ',';\n",
      "\n",
      "\n",
      "COPY staging_temp FROM 's3://deng-capstone/final_csv/valid_temp_data/part' \n",
      "CREDENTIALS 'aws_iam_role=arn:aws:iam::243965846266:role/dwhRole'\n",
      "csv REGION 'us-west-2'\n",
      "delimiter ',';\n",
      "\n",
      "\n",
      "COPY staging_air FROM 's3://deng-capstone/final_csv/valid_air_data/part' \n",
      "CREDENTIALS 'aws_iam_role=arn:aws:iam::243965846266:role/dwhRole'\n",
      "csv REGION 'us-west-2'\n",
      "delimiter ',';\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# execute the redshift query\n",
    "from psycopg2 import Error\n",
    "for query in staging_queries:\n",
    "    try:\n",
    "        print(query)\n",
    "        cur.execute(query)\n",
    "    except Error as e:\n",
    "        print(e.diag.message_primary)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dimension table loading queries\n",
    "insert_dem = (\"\"\"\n",
    "INSERT INTO demographics(city, state, median_age, male_population, female_population,\n",
    "total_population, number_of_veterans, foreign_born,\n",
    "average_household_size, state_code, race, count)\n",
    "(\n",
    "SELECT city, state, median_age, male_population, female_population,\n",
    "total_population, number_of_veterans, foreign_born,\n",
    "average_household_size, state_code, race, count\n",
    "FROM staging_dem\n",
    ")\"\"\")\n",
    "\n",
    "insert_temp = (\"\"\"\n",
    "INSERT INTO tempuratures(datetime, average_temperature , average_temperature_uncertainty, city,\n",
    "country, latitude, longitude)\n",
    "( SELECT * FROM staging_temp )\"\"\")\n",
    "\n",
    "insert_air = (\"\"\"\n",
    "INSERT INTO airports(airport_id, type, name, elevation_ft, continent, iso_country,\n",
    "iso_region, city, gps_code, iata_code, local_code, coordinates)\n",
    "(\n",
    "SELECT airport_id, type, name, elevation_ft, continent, iso_country,\n",
    "iso_region, city, gps_code, iata_code, local_code, coordinates\n",
    "FROM staging_air\n",
    ")\"\"\")\n",
    "\n",
    "insert_im = (\"\"\"\n",
    "INSERT INTO immigration(dem_id, temp_id, airport_id, \n",
    "cicid, year, month, city_code,\n",
    "res_code, entry_city, arrival_date, arrival_mode, state, \n",
    "dep_date, age, visa_type, count, dtadfile, visapost, \n",
    "occupation, entdepa, entdepd, entdepu, matflag, birth_year, \n",
    "dtaddto, gender, insnum, airline, admission_num, fltno, visatype, full_entry_city)\n",
    "(\n",
    "SELECT d.dem_id, t.temp_id, a.airport_id, i.cicid, i.year, i.month, i.city_code, i.res_code, i.entry_city , i.arrival_date, i.arrival_mode, i.state, \n",
    "i.dep_date, i.age, i.visa_type, i.count, i.dtadfile, i.visapost, i.occupation, i.entdepa, i.entdepd , i.entdepu , i.matflag, i.birth_year, \n",
    "i.dtaddto, i.gender, i.insnum, i.airline, i.admission_num, i.fltno, i.visatype, i.full_entry_city\n",
    "FROM staging_img i\n",
    "LEFT JOIN demographics d ON i.entry_city = d.city\n",
    "LEFT JOIN tempuratures t ON i.entry_city = t.city\n",
    "LEFT JOIN airports a ON i.entry_city = a.city\n",
    ")\"\"\")\n",
    "\n",
    "# make sure all dim tables added\n",
    "insert_queries = [insert_air, insert_dem, insert_temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSERT INTO airports(airport_id, type, name, elevation_ft, continent, iso_country,\n",
      "iso_region, city, gps_code, iata_code, local_code, coordinates)\n",
      "(\n",
      "SELECT airport_id, type, name, elevation_ft, continent, iso_country,\n",
      "iso_region, city, gps_code, iata_code, local_code, coordinates\n",
      "FROM staging_air\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# execute dimension table loading queries\n",
    "for query in insert_queries:\n",
    "    print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute fact table loading query\n",
    "query = insert_im\n",
    "cur.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.ch7ro1wfowik.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>397759</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(397759,)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SELECT COUNT(*) FROM immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nyc_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-438c6be87967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The number of records in the immigration data frame: {nyc_valid.count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nyc_valid' is not defined"
     ]
    }
   ],
   "source": [
    "# df counts\n",
    "print(f\"The number of records in the immigration data frame: {nyc_valid.count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immigration counts for the month grouped by date\n",
    "# tempurature for nyc day to day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dictionaries are in the data_dictionary folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary tools used in this project are Spark, Pandas, S3 for the data lake and Redshift for the dimensional model. With respect to data processing and exploration, Spark is a key tool as it provides the parallelized abstraction of the Spark data frame and operates on that parallelized data structure in memory, making the ETL process much faster than traditional data processing tools such as MapReduce. When it comes to visualizing the data, Pandas is used because it integrates well with jupyter notebooks making the tables look much cleaner than simple df.show() on a spark data frame. S3 is used as the data lake because it provides an easy to use interface for storing lots of files and the cost is relativley low. On top of this, it has good integration with Redshift and allows the loading to be done in parallel just like spark allowed for the transformations. Redshift is used because as a column based database that can be interfaced with using postgres, it provides both the query speed that a data ware house needs and the familiarity of PostgreSQL, allowing existing SQL based workloads such as those an analyst might use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often the data should be updated depends on the use for this pipeline. The scope of this project prepared a data set for examining possible relationships between tempurature and immigration. If this information is to be used for long term urban planning projects, less frequent updates, such as daily throughout the project's life, would suffice. If however, the data set will be used for operational decision that need to be made faster, such as new immigration decisions, it must be updated much more frequently such as hourly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data was increased 100x, I would have to be much more careful about operating on it and tracking changes. In all liklihood I would load the data set into S3 first to free up on prem space requirements. I would also increase the amount of nodes in the redshift cluster to handle the extra data loading.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, I went through this project keeping in mind that someone will need to use to it. This is why I chose to use Redshift. Since it can interfaced with using PostgreSQL, pretty much any analyst or BI specialist - of which SQL knowledge is a basic requirement - can interact with the data. In addition, because of the popularity of PostgreSQL, most popular dashboard tools like Tableau, Power BI etc. will likely have native, easy to use support for things like connectors to PostgreSQL databases. The fact that it would need to be updated at a specific time each day would make an scheduling tool like airflow more necessary and would handle running all of the code in this notebook before the dashboard needs to be examined each day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data needed to be accessed by 100+ people, Redshift configuration can be changed to make it more available. Some examples of that would be having Redshift servicing multiple availability zones. In addition, if there are common workloads or queries that need to be run by many of these people, they can be prepared in advanced such as pre aggregated OLAP cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
